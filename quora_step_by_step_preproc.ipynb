{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quora Insincere Questions Classification\n",
    "\n",
    "## Introduction\n",
    "We want to classify quora question (one sentence mainly) into 2 classes\n",
    "* Normal question\n",
    "* Toxic trolling\n",
    "\n",
    "To do so, we have a train / testing (tests not labeled) of 1306142 training questions (0/1 labels) and 56372 tests (no labels). On the 1306142 training data, around 80810 are labeled as trolling question and the remaining 1225312 are not. It brings us a pretty unbalanced data set (might want to use weight balancing on that: https://www.kdnuggets.com/2018/12/handling-imbalanced-datasets-deep-learning.html).\n",
    "\n",
    "## Ideas\n",
    "Things I want to try:\n",
    "* weight balancing (see above);\n",
    "* specializing projection on training data (will need a small enough model to specialize, glove is humongous)\n",
    "* cross-validation on hyperparameters (probably not...);\n",
    "* finding similar problems with :\n",
    " * label emotion detection;\n",
    " * etc\n",
    "* A model progressively refining his idea of the class while reading the sentence, word by word\n",
    "* Taking the average projection of sentence make us lose the length of the sentence as a variable\n",
    "* Two layers embedding to capture more semantic content\n",
    "\n",
    "## Data Preprocessing\n",
    "\n",
    "### Training / Test Data Split\n",
    "To start we are going to split the traing data in training / testing sets, and project them over one of the given projection. We will keep the same ratio of normal / toxic data between training and testing sets.\n",
    "\n",
    "Analysing the data shows that some lines have weird, hard to parse structure. Most of them are of type:\n",
    "* *id,\"Question\",[01]*\n",
    "But some questions go on several lines with intricated \"\"\". Well, let's hope pandas can deal with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "datadir = \"/media/marc/DATA/DATA/kaggle/01-quora\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1306122, 3)\n",
      "In \"Star Trek 2013\" why did they :\n",
      "\n",
      "*Spoilers*\n",
      "*Spoilers*\n",
      "*Spoilers*\n",
      "*Spoilers*\n",
      "\n",
      "1)Make warping look quite a bit like an hyperspace jump\n",
      "2)what in the world were those bright particles as soon as they jumped.\n",
      "3)Why in the world did they make it possible for two entities to react in warp space in separate jumps.\n",
      "4)Why did Spock get emotions for this movie.\n",
      "5)What was the point of hiding the \"Enterprise\" underwater.\n",
      "6)When they were intercepted by the dark ship, how come they reached Earth when they were far away from her.(I don't seem to remember the scene where they warp to earth).\n",
      "7)How did the ship enter earth's atmosphere when it wasnt even in orbit.\n",
      "8)When Scotty opened the door of the black ship , how come pike and khan didn't slow down?\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(os.path.join(datadir,\"train.csv\"))\n",
    "print(data.shape)\n",
    "print(data.iloc[522266][\"question_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It sems to be OK with long descriptions. Using spaCy or NLTK should do the job on the text.\n",
    "\n",
    "Let's split our data in train / test samples. Let's say 90 / 10 split (we only have 80000 trolling data, so let's not have a too short testing set).\n",
    "First we isolate the trolling data from the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of troll data = 80810\n",
      "Amount of genuine data = 1225312\n"
     ]
    }
   ],
   "source": [
    "troll_mask = np.array(data[\"target\"] == 1)\n",
    "print(\"Amount of troll data = {}\".format(np.sum(troll_mask)))\n",
    "troll_data = data[[\"qid\",\"question_text\",\"target\"]][troll_mask]\n",
    "genu_mask = np.logical_not(troll_mask)\n",
    "print(\"Amount of genuine data = {}\".format(np.sum(genu_mask)))\n",
    "genu_data = data[[\"qid\",\"question_text\",\"target\"]][genu_mask]\n",
    "assert data.shape[0] == troll_data.shape[0] + genu_data.shape[0],\"Kind data + troll data != total number\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's shuffle the data\n",
    "troll_shuff = troll_data.sample(n=troll_data.shape[0],random_state=1).reset_index(drop=True)\n",
    "genu_shuff = genu_data.sample(n=genu_data.shape[0],random_state=1).reset_index(drop=True)\n",
    "\n",
    "assert troll_shuff.shape[0] + genu_shuff.shape[0] == data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally we fuse the data into independent training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape = (1175509, 3)\n",
      "Test data shape = (130613, 3)\n"
     ]
    }
   ],
   "source": [
    "train_prop = 0.9\n",
    "\n",
    "# troll data\n",
    "train_part = int(troll_shuff.shape[0]*train_prop)\n",
    "train_data = troll_shuff[:train_part][:]\n",
    "test_data = troll_shuff[train_part:][:]\n",
    "assert troll_shuff.shape[0] == train_data.shape[0] + test_data.shape[0]\n",
    "\n",
    "# add the genuine data (note the index reseting)\n",
    "train_part = int(genu_shuff.shape[0]*train_prop)\n",
    "train_data = train_data.append(genu_shuff[:train_part][:]).reset_index(drop=True)\n",
    "test_data = test_data.append(genu_shuff[train_part:][:]).reset_index(drop=True)\n",
    "\n",
    "# Check out resulting size\n",
    "print(\"Train data shape = {}\".format(train_data.shape))\n",
    "print(\"Test data shape = {}\".format(test_data.shape))\n",
    "assert train_data.shape[0] + test_data.shape[0] == data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we finally shuffle the data one last time and save them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.sample(n=train_data.shape[0],random_state=1).reset_index(drop=True)\n",
    "test_data = test_data.sample(n=test_data.shape[0],random_state=1).reset_index(drop=True)\n",
    "train_data.to_csv(\"training_90.csv\",index=False)\n",
    "test_data.to_csv(\"test_90.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Projection\n",
    "Next step is to project our data on one of the given vector space. We are going to start with Glove and reach all the way to a proper classifier before trying other space or specializing one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data\n",
    "train_data = pd.read_csv(\"training_90.csv\")\n",
    "test_data = pd.read_csv(\"test_90.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.08 s, sys: 31.1 ms, total: 4.12 s\n",
      "Wall time: 4.11 s\n"
     ]
    }
   ],
   "source": [
    "# projecion data\n",
    "def load_space(filename):\n",
    "    space = {}\n",
    "    i = 1\n",
    "    with open(filename,\"rt\") as f:\n",
    "        for line in f:\n",
    "            word,*coeff = line.split()\n",
    "            try:\n",
    "                space[word] = np.array(coeff,dtype=float)\n",
    "            except ValueError as err:\n",
    "                print(\"Error \\\"{}\\\" at line {}\".format(err,i))\n",
    "            i+=1\n",
    "    # reshaping (because (size,))\n",
    "    vsize = next(iter(space.values())).shape[0]\n",
    "    for word in space.keys():\n",
    "        space[word] = space[word].reshape(vsize,1)\n",
    "    return space\n",
    "\n",
    "%time space = load_space(\"glove/glove.6B.50d.txt\") # let's try with something small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elements in space: 400000\n",
      "Projection shape: (50, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"Elements in space: {}\".format(len(space)))\n",
    "print(\"Projection shape: {}\".format(space[\"to\"].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we project sentences, word by word, in this space. In our first naive approach, we will take the mean of all the vectors in the sentence as an input.\n",
    "Let's try with a simple sentence of the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why do some Quora writers need to write War and Peace, when a terse answer would do?\n",
      "(0.32386 + 0.29605 + 0.92871) / 3 = 0.26804880000000003\n",
      "(0.011154 + -0.13841 + -0.10834) / 3 = 0.02078619999999998\n",
      "(0.23443 + 0.043774 + 0.21497) / 3 = 0.02956493333333334\n",
      "(-0.18039 + -0.38744 + -0.50237) / 3 = -0.27284440000000004\n",
      "(0.6233 + 0.12262 + 0.10379) / 3 = 0.24397293333333334\n"
     ]
    }
   ],
   "source": [
    "def project_sentence(sent,space):\n",
    "    sentence = sent.lower() # all these projection sets are lower case only\n",
    "    vsize = next(iter(space.values())).shape[0]\n",
    "    proj = np.zeros((vsize,1))\n",
    "    count = 0\n",
    "    for word in sentence.split():\n",
    "        if word in space:\n",
    "            proj += space[word]\n",
    "            count +=1\n",
    "    if count != 0:\n",
    "        proj /= count\n",
    "    return proj\n",
    "\n",
    "# quick check\n",
    "sentence = train_data.sample(n=1,random_state=1)[\"question_text\"].values[0]\n",
    "proj = project_sentence(sentence,space)\n",
    "print(sentence)\n",
    "sentence = sentence.lower()\n",
    "words = sentence.split()\n",
    "for i in range(min(5,len(words))):\n",
    "    print(\"({} + {} + {}) / 3 = {}\".format(space[words[0]][i][0],space[words[1]][i][0],space[words[2]][i][0],proj[i][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_compete' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-152-3769ccf1deab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# and save\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mtrain_complete\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"training_90p_50d.csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mtest_compete\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test_90p_50d.csv\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'test_compete' is not defined"
     ]
    }
   ],
   "source": [
    "vsize = next(iter(space.values())).shape[0]\n",
    "\n",
    "# Project training data\n",
    "projections = np.zeros((train_data.shape[0],vsize,1))\n",
    "for i in range(train_data.shape[0]):\n",
    "    sentence = train_data.iloc[i][\"question_text\"]\n",
    "    projections[i] = project_sentence(sentence,space)\n",
    "\n",
    "# Save in a complete dataframe\n",
    "train_complete = train_data.copy(deep=True)\n",
    "train_complete[\"projections\"] = None\n",
    "for i in range(projections.shape[0]):\n",
    "    train_complete.at[i,\"projections\"] = projections[i]\n",
    "\n",
    "# project test data\n",
    "projections = np.zeros((test_data.shape[0],vsize,1))\n",
    "for i in range(test_data.shape[0]):\n",
    "    sentence = test_data.iloc[i][\"question_text\"]\n",
    "    projections[i] = project_sentence(sentence,space)\n",
    "\n",
    "# Save in a complete dataframe\n",
    "test_complete = test_data.copy(deep=True)\n",
    "test_complete[\"projections\"] = None\n",
    "for i in range(projections.shape[0]):\n",
    "    test_complete.at[i,\"projections\"] = projections[i]\n",
    "    \n",
    "# and save\n",
    "train_complete.to_csv(\"training_90p_50d.csv\",index=False)\n",
    "test_complete.to_csv(\"test_90p_50d.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
