{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quora Insincere Questions Classification\n",
    "# Part 1: Preprocessing\n",
    "\n",
    "## Introduction\n",
    "We want to classify quora question (one sentence mainly) into 2 classes\n",
    "* Normal question\n",
    "* Toxic trolling\n",
    "\n",
    "To do so, we have a train / testing (tests not labeled) of 1306142 training questions (0/1 labels) and 56372 tests (no labels). On the 1306142 training data, around 80810 are labeled as trolling question and the remaining 1225312 are not. It brings us a pretty unbalanced data set (might want to use weight balancing on that: https://www.kdnuggets.com/2018/12/handling-imbalanced-datasets-deep-learning.html).\n",
    "\n",
    "## Ideas\n",
    "Things I want to try:\n",
    "* weight balancing (see above);\n",
    "* specializing projection on training data (will need a small enough model to specialize, glove is humongous)\n",
    "* cross-validation on hyperparameters (probably not...);\n",
    "* finding similar problems with :\n",
    " * label emotion detection;\n",
    " * etc\n",
    "* A model progressively refining his idea of the class while reading the sentence, word by word\n",
    "* Taking the average projection of sentence make us lose the length of the sentence as a variable\n",
    "* Two layers embedding to capture more semantic content\n",
    "* Put Glove data in SQL BDD for memory manageable projections\n",
    "* Note to self: python multithreading is still garbage in practice.\n",
    "\n",
    "## Data Preprocessing\n",
    "\n",
    "### Training / Test Data Split\n",
    "To start we are going to split the traing data in training / testing sets, and project them over one of the given projection. We will keep the same ratio of normal / toxic data between training and testing sets.\n",
    "\n",
    "Analysing the data shows that some lines have weird, hard to parse structure. Most of them are of type:\n",
    "* *id,\"Question\",[01]*\n",
    "But some questions go on several lines with intricated \"\"\". Well, let's hope pandas can deal with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "import src.quora.preproc as pp\n",
    "\n",
    "model_path = \"data/embedded\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1306122, 3)\n",
      "In \"Star Trek 2013\" why did they :\n",
      "\n",
      "*Spoilers*\n",
      "*Spoilers*\n",
      "*Spoilers*\n",
      "*Spoilers*\n",
      "\n",
      "1)Make warping look quite a bit like an hyperspace jump\n",
      "2)what in the world were those bright particles as soon as they jumped.\n",
      "3)Why in the world did they make it possible for two entities to react in warp space in separate jumps.\n",
      "4)Why did Spock get emotions for this movie.\n",
      "5)What was the point of hiding the \"Enterprise\" underwater.\n",
      "6)When they were intercepted by the dark ship, how come they reached Earth when they were far away from her.(I don't seem to remember the scene where they warp to earth).\n",
      "7)How did the ship enter earth's atmosphere when it wasnt even in orbit.\n",
      "8)When Scotty opened the door of the black ship , how come pike and khan didn't slow down?\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"data/train.csv\")\n",
    "print(data.shape)\n",
    "print(data.iloc[522266][\"question_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems OK.\n",
    "Let's split our data in train / test samples. Let's say 90 / 10 split (we only have 80000 trolling data, so let's not have a too short testing set). pp.split_quora_csv will do the job. This function keeps the same proportion of troll data in both training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of troll data = 80810\n",
      "Amount of genuine data = 1225312\n",
      "Train data shape = (1175509, 3)\n",
      "Test data shape = (130613, 3)\n",
      "Saved in training_90.csv and test_90.csv\n"
     ]
    }
   ],
   "source": [
    "import src.quora.preproc as pp\n",
    "pp.split_quora_csv(\"data/train.csv\",train_prop=0.9,\n",
    "                   output_train=os.path.join(model_path,\"training_90.csv\"),\n",
    "                   output_test=os.path.join(model_path,\"test_90.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick grep on both file confirm that they have a proportion of troll inputs of 16.16%\n",
    "\n",
    "### Data Projection\n",
    "Next step is to project our data on one of the given vector space. We are going to start with Glove and reach all the way to a proper classifier before trying other space or specializing one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elements in space: 400000\n",
      "Projection shape: (50, 1)\n"
     ]
    }
   ],
   "source": [
    "# Loading data\n",
    "train_data = pd.read_csv(os.path.join(model_path,\"training_90.csv\"))\n",
    "test_data = pd.read_csv(os.path.join(model_path,\"test_90.csv\"))\n",
    "\n",
    "# Loading glove projections in a dictionary\n",
    "space = pp.load_glove(\"glove/glove.6B.50d.txt\")\n",
    "print(\"Elements in space: {}\".format(len(space)))\n",
    "print(\"Projection shape: {}\".format(next(iter(space.values())).shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we project sentences, word by word, in this space. In our first naive approach, we will take the mean of all the vectors in the sentence as an input.\n",
    "Let's try with a simple sentence of the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like cats\n",
      "(0.11891 + 0.36808 + 0.43082) / 3 = 0.3059366666666667\n",
      "(0.15255 + 0.20834 + -0.85216) / 3 = -0.1637566666666667\n",
      "(-0.082073 + -0.22319 + -0.55639) / 3 = -0.28721766666666665\n"
     ]
    }
   ],
   "source": [
    "#sentence = train_data.sample(n=1,random_state=1)[\"question_text\"].values[0]\n",
    "sentence = \"I like cats\"\n",
    "proj = pp.project_sentence(sentence,space)\n",
    "print(sentence)\n",
    "sentence = sentence.lower()\n",
    "words = sentence.split()\n",
    "for i in range(min(5,len(words))):\n",
    "    print(\"({} + {} + {}) / 3 = {}\".format(\n",
    "        space[words[0]][i][0],space[words[1]][i][0],space[words[2]][i][0],proj[i][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems to work well. \n",
    "\n",
    "Let's now project the whole set and save it. project_data runs in parallel using by default 4 threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 44s, sys: 224 ms, total: 2min 45s\n",
      "Wall time: 2min 45s\n"
     ]
    }
   ],
   "source": [
    "# For training data first\n",
    "%time train_proj = pp.project_data(train_data,space)\n",
    "pp.save_projections(train_data[\"qid\"],train_proj,train_data[\"target\"],os.path.join(model_path,\"training_90p_50d.proj\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18 s, sys: 27.5 ms, total: 18.1 s\n",
      "Wall time: 18.1 s\n"
     ]
    }
   ],
   "source": [
    "# For test data\n",
    "%time test_proj = pp.project_data(test_data,space)\n",
    "pp.save_projections(test_data[\"qid\"],test_proj,test_data[\"target\"],os.path.join(model_path,\"test_90p_50d.proj\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And data are projected and saved. Next will be to connect them to a Neural Network model.\n",
    "For the sake of fast prototyping and testing, we also save a reduced model of only 50,000 elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global ratio = 0.06187021962400968\n",
      "50000 ratio = 0.05996\n"
     ]
    }
   ],
   "source": [
    "# let's check how many 1s are in the 50000 elements\n",
    "nb_el = 50000\n",
    "print(\"Global ratio = {}\".format(sum(train_data[\"target\"][:])/train_data.shape[0]))\n",
    "print(\"{} ratio = {}\".format(nb_el,sum(train_data[\"target\"][:nb_el])/nb_el))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And let's save that\n",
    "pp.save_projections(train_data[\"qid\"][:nb_el],train_proj[:nb_el],train_data[\"target\"][:nb_el],\n",
    "                    os.path.join(model_path,\"training_90p_50d_{}.proj\".format(nb_el)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
