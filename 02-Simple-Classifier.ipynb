{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quora Insincere Questions Classification\n",
    "# Part 2: Classifier\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this part, we will create and tune several classifiers, and test different techniques such as weight balancing on the Quora Insincere Questions Classification challenge.\n",
    "Split and projected train and test data are already ready to use. \n",
    "\n",
    "## Simple Model\n",
    "Let's start with a simple model. No weight balancing, only one or two layers and no regularization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'src.quora.preproc' from '/home/marc/work/kaggle/01-quora/src/quora/preproc.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import src.quora.preproc as pp\n",
    "\n",
    "from importlib import reload\n",
    "\n",
    "reload(pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 50)\n",
      "(130613, 50)\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"data/embedded\"\n",
    "train_index,train_data, train_targets = pp.load_projections(os.path.join(data_dir,\"training_90p_50d_50000.proj\"))\n",
    "test_index,test_data,test_targets = pp.load_projections(os.path.join(data_dir,\"test_90p_50d.proj\"))\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_perceptron(X,input_dim):\n",
    "    \n",
    "    # Simple perceptron\n",
    "    W = tf.get_variable(\"W\",[1,input_dim],regularizer=tf.contrib.layers.l2_regularizer(0.01),\n",
    "                        initializer=tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    b = tf.get_variable(\"b\",[1,1],regularizer=tf.contrib.layers.l2_regularizer(0.01),\n",
    "                        initializer=tf.zeros_initializer())\n",
    "    \n",
    "    Z = tf.matmul(W,X) + b\n",
    "    #A = tf.sigmoid(Z,name=\"sig\")\n",
    "    \n",
    "    model_dict = {\"W\":W,\"b\":b,\"Z\":Z}\n",
    "        \n",
    "    return model_dict\n",
    "\n",
    "def run_perceptron(train,targets,epochs,epoch_print=500,learning_rate=0.001):\n",
    "    \n",
    "    # Input dim -> vector length (proj_dim,1), minibatch size is determined at runtime... I think\n",
    "    # We know output is a log regression so 1\n",
    "    X = tf.placeholder(name=\"X\",dtype=tf.float32,shape=[train.shape[0],None])\n",
    "    Y = tf.placeholder(name=\"Y\",dtype=tf.float32,shape=[1,None])\n",
    "    \n",
    "    model_dict = set_perceptron(X,train.shape[0])\n",
    "    \n",
    "    # Weighted loss function\n",
    "    ratio = 1-np.sum(targets)/targets.shape[1]\n",
    "    print(\"Label learning weight ration = {}\".format(ratio))\n",
    "    wp = tf.multiply(Y,tf.constant(ratio,dtype=tf.float32))\n",
    "    wn = tf.multiply(1-Y,tf.constant(1-ratio,dtype=tf.float32))\n",
    "    weights = tf.add(wp,wn)\n",
    "    #cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=model_dict[\"Z\"],labels=Y))\n",
    "    cost = tf.reduce_mean(tf.losses.sigmoid_cross_entropy(Y,model_dict[\"Z\"],weights=weights))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        for epoch in range(epochs):\n",
    "            _ , epoch_cost = sess.run([optimizer, cost], feed_dict={X:train, Y:targets})\n",
    "            if(epoch % epoch_print == 0 or epoch == epochs - 1):\n",
    "                print(\"Epoch: {}, cost={}\".format(epoch,epoch_cost))\n",
    "                correct_prediction = tf.equal(tf.round(tf.sigmoid(model_dict[\"Z\"])),Y)\n",
    "                accuracy = tf.reduce_mean(tf.cast(correct_prediction,\"float\"))\n",
    "                print(\"Accuracy = {}\".format(accuracy.eval({X:train,Y:targets})))\n",
    "                \n",
    "        parameters = {\"W\":model_dict[\"W\"],\"b\":model_dict[\"b\"]}\n",
    "        model = sess.run(parameters)\n",
    "        \n",
    "        # printing proportion of correctly classified by class\n",
    "        nb_p = np.sum(targets)\n",
    "        nb_n = np.sum(1-targets)\n",
    "        mask_p = targets.astype(bool)\n",
    "        mask_n = np.logical_not(mask_p)\n",
    "        predictions = tf.equal(tf.round(tf.sigmoid(model_dict[\"Z\"])),Y).eval({X:train,Y:targets})\n",
    "        print(\"positive ratio : {} / {} = {}\".format(np.sum(np.logical_and(mask_p,predictions)),\n",
    "                                                     nb_p,\n",
    "                                                     np.sum(np.logical_and(mask_p,predictions))/nb_p))\n",
    "        print(\"negative ratio : {} / {} = {}\".format(np.sum(np.logical_and(mask_n,predictions)),\n",
    "                                                     nb_n,\n",
    "                                                     np.sum(np.logical_and(mask_n,predictions))/nb_n))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    return model\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label learning weight ration = 0.94004\n",
      "Epoch: 0, cost=0.07986059039831161\n",
      "Accuracy = 0.12421999871730804\n",
      "Epoch: 1000, cost=0.05576595664024353\n",
      "Accuracy = 0.7840999960899353\n",
      "Epoch: 2000, cost=0.05272633954882622\n",
      "Accuracy = 0.7897400259971619\n",
      "Epoch: 3000, cost=0.051876455545425415\n",
      "Accuracy = 0.7906000018119812\n",
      "Epoch: 4000, cost=0.05162222310900688\n",
      "Accuracy = 0.7904999852180481\n",
      "Epoch: 4999, cost=0.05153188109397888\n",
      "Accuracy = 0.7910000085830688\n",
      "positive ratio : 2384 / 2998.0 = 0.7951967978652434\n",
      "negative ratio : 37166 / 47002.0 = 0.7907323092634356\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "model = run_perceptron(train_data.T,train_targets.reshape(1,len(train_targets)),5000,epoch_print=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best results after tunning it for a while is about 80% accuracy for both group. Not great.\n",
    "Next step is to create a deeper model with estimator or keras and see if we can do better.\n",
    "\n",
    "## Deeper Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
