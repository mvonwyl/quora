{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quora Insincere Questions Classification\n",
    "# Part 2: Classifier\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this part, we will create and tune several classifiers, and test different techniques such as weight balancing on the Quora Insincere Questions Classification challenge.\n",
    "Split and projected train and test data are already ready to use. \n",
    "\n",
    "## Simple Model\n",
    "Let's start with a simple model. No weight balancing, only one or two layers and no regularization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'src.quora.preproc' from '/home/marc/work/kaggle/01-quora/src/quora/preproc.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import src.quora.preproc as pp\n",
    "\n",
    "from importlib import reload\n",
    "\n",
    "reload(pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 50)\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"data/embedded\"\n",
    "train_index,train_data, train_targets = pp.load_projections(os.path.join(data_dir,\"training_90p_50d_50000.proj\"))\n",
    "print(train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_perceptron(X,input_dim):\n",
    "    \n",
    "    # Simple perceptron\n",
    "    W = tf.get_variable(\"W\",[1,input_dim],regularizer=tf.contrib.layers.l2_regularizer(0.01),\n",
    "                        initializer=tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    b = tf.get_variable(\"b\",[1,1],regularizer=tf.contrib.layers.l2_regularizer(0.01),\n",
    "                        initializer=tf.zeros_initializer())\n",
    "    \n",
    "    Z = tf.matmul(W,X) + b\n",
    "    #A = tf.sigmoid(Z,name=\"sig\")\n",
    "    \n",
    "    model_dict = {\"W\":W,\"b\":b,\"Z\":Z}\n",
    "        \n",
    "    return model_dict\n",
    "\n",
    "def run_perceptron(train,targets,epochs,epoch_print=500,learning_rate=0.0001):\n",
    "    \n",
    "    # Input dim -> vector length (proj_dim,1), minibatch size is determined at runtime... I think\n",
    "    # We know output is a log regression so 1\n",
    "    X = tf.placeholder(name=\"X\",dtype=tf.float32,shape=[train.shape[0],None])\n",
    "    Y = tf.placeholder(name=\"Y\",dtype=tf.float32,shape=[1,None])\n",
    "    \n",
    "    model_dict = set_perceptron(X,train.shape[0])\n",
    "    \n",
    "    # Weighted loss function\n",
    "    ratio = 1-np.sum(targets)/targets.shape[1]\n",
    "    print(\"Label learning weight ration = {}\".format(ratio))\n",
    "    wp = tf.multiply(Y,tf.constant(ratio,dtype=tf.float32))\n",
    "    wn = tf.multiply(1-Y,tf.constant(1-ratio,dtype=tf.float32))\n",
    "    weights = tf.add(wp,wn)\n",
    "    #cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=model_dict[\"Z\"],labels=Y))\n",
    "    cost = tf.reduce_mean(tf.losses.sigmoid_cross_entropy(Y,model_dict[\"Z\"],weights=weights))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        for epoch in range(epochs):\n",
    "            _ , epoch_cost = sess.run([optimizer, cost], feed_dict={X:train, Y:targets})\n",
    "            if(epoch % epoch_print == 0 or epoch == epochs - 1):\n",
    "                print(\"Epoch: {}, cost={}\".format(epoch,epoch_cost))\n",
    "                correct_prediction = tf.equal(tf.round(tf.sigmoid(model_dict[\"Z\"])),Y)\n",
    "                accuracy = tf.reduce_mean(tf.cast(correct_prediction,\"float\"))\n",
    "                print(\"Accuracy = {}\".format(accuracy.eval({X:train,Y:targets})))\n",
    "                \n",
    "        parameters = {\"W\":model_dict[\"W\"],\"b\":model_dict[\"b\"]}\n",
    "        model = sess.run(parameters)\n",
    "        \n",
    "        # printing proportion of correctly classified by class\n",
    "        nb_p = np.sum(targets)\n",
    "        nb_n = np.sum(1-targets)\n",
    "        mask_p = targets.astype(bool)\n",
    "        mask_n = np.logical_not(mask_p)\n",
    "        predictions = tf.equal(tf.round(tf.sigmoid(model_dict[\"Z\"])),Y).eval({X:train,Y:targets})\n",
    "        print(\"positive ratio : {} / {} = {}\".format(np.sum(np.logical_and(mask_p,predictions)),\n",
    "                                                     nb_p,\n",
    "                                                     np.sum(np.logical_and(mask_p,predictions))/nb_p))\n",
    "        print(\"negative ratio : {} / {} = {}\".format(np.sum(np.logical_and(mask_n,predictions)),\n",
    "                                                     nb_n,\n",
    "                                                     np.sum(np.logical_and(mask_n,predictions))/nb_n))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    return model\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label learning weight ration = 0.94004\n",
      "Epoch: 0, cost=0.07986059039831161\n",
      "Accuracy = 0.12421999871730804\n",
      "Epoch: 1000, cost=0.05576595664024353\n",
      "Accuracy = 0.7840999960899353\n",
      "Epoch: 2000, cost=0.05272633954882622\n",
      "Accuracy = 0.7897400259971619\n",
      "Epoch: 3000, cost=0.051876455545425415\n",
      "Accuracy = 0.7906000018119812\n",
      "Epoch: 4000, cost=0.05162222310900688\n",
      "Accuracy = 0.7904999852180481\n",
      "Epoch: 4999, cost=0.05153188109397888\n",
      "Accuracy = 0.7910000085830688\n",
      "positive ratio : 2384 / 2998.0 = 0.7951967978652434\n",
      "negative ratio : 37166 / 47002.0 = 0.7907323092634356\n",
      "CPU times: user 1min 47s, sys: 8.17 s, total: 1min 55s\n",
      "Wall time: 1min 22s\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "%time little_model = run_perceptron(train_data.T,train_targets.reshape(1,len(train_targets)),5000,epoch_print=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best results after tunning it for a while is about 80% accuracy for both group. Not great. Let's train on the whole dataset and test on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1175509, 50)\n",
      "(130613, 50)\n"
     ]
    }
   ],
   "source": [
    "train_index,train_data, train_targets = pp.load_projections(os.path.join(data_dir,\"training_90p_50d.proj\"))\n",
    "test_index,test_data,test_targets = pp.load_projections(os.path.join(data_dir,\"test_90p_50d.proj\"))\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label learning weight ration = 0.9381297803759903\n",
      "Epoch: 0, cost=0.0822649747133255\n",
      "Accuracy = 0.12650689482688904\n",
      "Epoch: 200, cost=0.06896921247243881\n",
      "Accuracy = 0.7215036153793335\n",
      "Epoch: 400, cost=0.06376713514328003\n",
      "Accuracy = 0.7654828429222107\n",
      "Epoch: 600, cost=0.060645777732133865\n",
      "Accuracy = 0.7777805328369141\n",
      "Epoch: 800, cost=0.05857668071985245\n",
      "Accuracy = 0.783149242401123\n",
      "Epoch: 1000, cost=0.05714581534266472\n",
      "Accuracy = 0.7861751914024353\n",
      "Epoch: 1200, cost=0.05612807348370552\n",
      "Accuracy = 0.7883895635604858\n",
      "Epoch: 1400, cost=0.05538638308644295\n",
      "Accuracy = 0.7894461154937744\n",
      "Epoch: 1600, cost=0.054834283888339996\n",
      "Accuracy = 0.7907493710517883\n",
      "Epoch: 1800, cost=0.054416149854660034\n",
      "Accuracy = 0.7915719747543335\n",
      "Epoch: 1999, cost=0.05409679189324379\n",
      "Accuracy = 0.7921623587608337\n",
      "positive ratio : 57236 / 72729.0 = 0.7869763093126538\n",
      "negative ratio : 873958 / 1102780.0 = 0.7925043979760242\n",
      "CPU times: user 20min 27s, sys: 7min 27s, total: 27min 54s\n",
      "Wall time: 27min 48s\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "%time model = run_perceptron(train_data.T,train_targets.reshape(1,len(train_targets)),2000,epoch_print=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Little model saved under data/little_model/little_model.ckpt\n",
      "Model saved under data/model/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "# Save models\n",
    "little_model_path = \"data/little_model/little_model.ckpt\"\n",
    "W = tf.Variable(little_model.get(\"W\"))\n",
    "b = tf.Variable(little_model.get(\"b\"))\n",
    "saver = tf.train.Saver({\"W\":W,\"b\":b})\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    spath = saver.save(sess,little_model_path)\n",
    "    print(\"Little model saved under {}\".format(spath))\n",
    "    \n",
    "model_path = \"data/model/model.ckpt\"\n",
    "W = tf.Variable(model.get(\"W\"))\n",
    "b = tf.Variable(model.get(\"b\"))\n",
    "saver = tf.train.Saver({\"W\":W,\"b\":b})\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    spath = saver.save(sess,model_path)\n",
    "    print(\"Model saved under {}\".format(spath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from data/little_model/little_model.ckpt\n",
      "W = [[-1.5350993  -1.0646862   0.93980783 -0.05377308  1.1097102   2.1315331\n",
      "   0.16534835  0.10330456 -0.9122293  -0.01724127 -1.2123718  -0.23289846\n",
      "   0.9106384  -0.39506698 -1.0739824  -0.50956106  0.9283245  -0.23896891\n",
      "   0.1469524   1.4991794  -0.48674285  1.9710633  -0.31396177  0.19180572\n",
      "  -0.47586817 -0.9097925   0.2967707   1.6963593  -0.35719016 -0.14206137\n",
      "  -0.6773655   1.2112517  -1.3445208   0.53827786 -1.1259295  -1.5763471\n",
      "   0.5442923  -1.4968342  -1.970133   -0.63807523 -0.91635275 -0.5067819\n",
      "   1.0440195   1.0228881   1.0398076  -0.19438055 -2.1268923  -1.2733794\n",
      "   0.11653239 -1.4713463 ]]\n",
      "b = [[0.27551776]]\n",
      "INFO:tensorflow:Restoring parameters from data/model/model.ckpt\n",
      "W = [[-1.0220009e+00 -7.4180824e-01  4.7776654e-01 -6.3137639e-01\n",
      "   1.0705887e+00  1.1300802e+00  2.7454108e-01  2.4078566e-01\n",
      "  -7.3606294e-01 -3.4437899e-02 -7.6920635e-01 -8.6861145e-01\n",
      "   4.7860834e-01  7.5270870e-04 -5.8917177e-01 -1.4161059e-01\n",
      "   5.8055645e-01 -5.6438243e-01  1.9516426e-01  1.0944934e+00\n",
      "  -9.2981666e-01  1.2370420e+00  3.4316328e-01  2.3726234e-01\n",
      "  -5.9148306e-01 -4.5132065e-01  5.3658694e-01  4.7523803e-01\n",
      "  -3.1089988e-01 -2.0105118e-01 -3.3446130e-01  5.4965216e-01\n",
      "  -9.9235737e-01  4.3021491e-01 -1.0440798e+00 -1.3004760e+00\n",
      "  -2.2477905e-01 -8.9682496e-01 -1.3773540e+00 -3.7123585e-01\n",
      "  -1.0044525e+00  5.2100676e-01  8.5150588e-01  4.5943579e-01\n",
      "   1.1179098e+00 -7.3704362e-01 -1.4288412e+00 -1.1212462e+00\n",
      "   6.8487436e-01 -1.1051533e+00]]\n",
      "b = [[0.04179015]]\n"
     ]
    }
   ],
   "source": [
    "# Check saved model\n",
    "tf.reset_default_graph()\n",
    "W = tf.get_variable(name=\"W\",shape=little_model.get(\"W\").shape)\n",
    "b = tf.get_variable(name=\"b\",shape=little_model.get(\"b\").shape)\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess,little_model_path)\n",
    "    print(\"W = %s\" % W.eval())\n",
    "    print(\"b = %s\" % b.eval())\n",
    "    \n",
    "tf.reset_default_graph()\n",
    "W = tf.get_variable(name=\"W\",shape=model.get(\"W\").shape)\n",
    "b = tf.get_variable(name=\"b\",shape=model.get(\"b\").shape)\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess,model_path)\n",
    "    print(\"W = %s\" % W.eval())\n",
    "    print(\"b = %s\" % b.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "None values not supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-b1977a8ce1e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Z\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtest_targets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"float\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy = {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnb_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnb_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36msigmoid\u001b[0;34m(x, name)\u001b[0m\n\u001b[1;32m   2280\u001b[0m   \"\"\"\n\u001b[1;32m   2281\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Sigmoid\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2282\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"x\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2283\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, preferred_dtype)\u001b[0m\n\u001b[1;32m   1048\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m       \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1050\u001b[0;31m       as_ref=False)\n\u001b[0m\u001b[1;32m   1051\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, ctx)\u001b[0m\n\u001b[1;32m   1144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1145\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1146\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1148\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    227\u001b[0m                                          as_ref=False):\n\u001b[1;32m    228\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name, verify_shape)\u001b[0m\n\u001b[1;32m    206\u001b[0m   tensor_value.tensor.CopyFrom(\n\u001b[1;32m    207\u001b[0m       tensor_util.make_tensor_proto(\n\u001b[0;32m--> 208\u001b[0;31m           value, dtype=dtype, shape=shape, verify_shape=verify_shape))\n\u001b[0m\u001b[1;32m    209\u001b[0m   \u001b[0mdtype_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m   const_tensor = g.create_op(\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/framework/tensor_util.py\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[0;34m(values, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    428\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"None values not supported.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m     \u001b[0;31m# if dtype is provided, forces numpy array to be the type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m     \u001b[0;31m# provided if possible.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: None values not supported."
     ]
    }
   ],
   "source": [
    "predictions = tf.equal(tf.round(tf.sigmoid(model.get(\"Z\")),Y).eval({X:test_data[:10],Y:test_targets[:10]}))\n",
    "accuracy = tf.reduce_mean(tf.cast(prediction,\"float\"))\n",
    "print(\"Accuracy = {}\".format(accuracy))\n",
    "nb_p = np.sum(targets)\n",
    "nb_n = np.sum(1-targets)\n",
    "mask_p = targets.astype(bool)\n",
    "mask_n = np.logical_not(mask_p)\n",
    "print(\"positive ratio : {} / {} = {}\".format(np.sum(np.logical_and(mask_p,predictions)),\n",
    "                                             nb_p,\n",
    "                                             np.sum(np.logical_and(mask_p,predictions))/nb_p))\n",
    "print(\"negative ratio : {} / {} = {}\".format(np.sum(np.logical_and(mask_n,predictions)),\n",
    "                                             nb_n,\n",
    "                                             np.sum(np.logical_and(mask_n,predictions))/nb_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['W', 'b'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
